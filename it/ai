Turing test: whether natural language conversation can differentiate machine from human
Superintelligence: exceed human performance in all domains
Artificial general intelligence (AGI): matches human cognitive capabilities
Intelligent agent: perceives environment and takes actions to achieve goal
Instrumental convergence: all intelligent agents pursu same sub-goals (self-preservation, resource acquisition)
AI alignment: prevent AI from harmful or unintended ways to achieve its goals
- Reward hacking: exploit loophole to achieve goal

#Latent variable model
Diffusion model: forward process + reverse process + sampling procedure
- image generation

=Large Language Model (LLM)
- Chain-of-thought
- Least-to-most
Transformer: multi-head attention mechanism

#Machine learning
Feature learning: automatically discover representation for classification of data

K-means clustering
Neural network: layers from input to output, weighted edges between layers
- Input layer
- Hidden layer
- Output layer
Principal component analysis (PCA): dimensionality reduction in exploratory data analysis
- principal component is combination of variables, components are uncorrelated
- puts maximum information in first component, maximum remaining in second etc
- finds line with most spread out projection of data points to line

#Probabilistic classifiers
Naive Bayes classifier

#Overfitting: model with too many parameters, fits closely to training data but fails on additional data
Generalization error: performance on additional data
Cross-validation: splitting data to train and test sets
Regularization: penalize more complex models
Early stopping: train until performance on test data stops improving

Loss function: measurement of model predictions accuracy, smaller is better
Gradient: vector field with values in direction of fastest increase, dimensions same as loss function
- Gradient descent: optimization method, finds local minimum by iterative steps in direction opposite of gradient
- Learning rate: determines step size of parameter updates during learning

Training: learning, minimize errors on training data
Inference: doing, use trained model to generate outputs

#Neural network
Activation function: introduces non-linearity, transforms weighted sum of inputs to output
Bias: constant added to weighted sum of inputs, increases flexibility
Backpropagation: allows learning, adjusts weights and biases to minimize error (expected vs real output)
- computes gradient of loss function in weight space
Attention
- Query: element of interest
- Key: input data
- Value: content data
- Attention weights: sum up to 1
- Multi-head attention: captures different aspects

Stochastic gradient descent (SGD): optimization method
Feedforward neural network (FNN): unidirectional neural network, usually fully connected (classification)
Convolutional neural network (CNN): feedforward neural network, regularized weights over fewer connections (image)
Recurrent neural network (RNN): bidirectional neural network (speech)
- Long short-term memory (LSTM): recurrent neural network with memory
Transformer: self-attention, parallel processing
- Large Language Model (LLM): (content generation, summarization, translation)
Neural Turing machine: recurrent neural network with external memory
